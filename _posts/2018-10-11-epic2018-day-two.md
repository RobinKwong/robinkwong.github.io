---
layout: post
title: Epic 2018 day 2 conference notes
date: 2018-10-11 07:00
tags: [conference, ethnography]
permalink: /epic2018-day-two/
---

![](/images/epic2018/epiclogo.JPG)

- [Notes for day one](/epic2018-day-one/)

## Design & Data

_What does a data expert see when they look at a design problem? This panel will immerse us in the practices of two data experts, both of whom have collaborated with ethnographers, as they navigate through design challenges in different ways. Chair Jeanette Blomberg will draw the panelists and audience into conversation about synergies and challenges for interdisciplinary design collaborations._

![](/images/epic2018/panellists.JPG)

### Thomas Y Lee

Introducing innovation and product design as a process

![](/images/epic2018/linear.JPG)

Whether it's design thinking, product design or hypotehsis dirven devleopment, at a high level the process is fundamentally the same.

Rather than thinking of this as a linear or a stage-gate process, in practice it is often circular and contiuous

![](/images/epic2018/circular.JPG)

So, the question becomes - what is the role of data in this cycle?

Data has long had a role but traditionally this is afterwards - A/B testing, pre/post, etc. In all of these different frameworks, data fundamentally answers questions of how many or how much.

Textual/sentiment analysis for example use word baskets, which are ultimately just counting words.

In order to develop an intimate understanding of a problem, we really need to ask uestions about **'why'**

So, the question is:

"How can we apply processes related to data to begin to answer questions about why and how?"

Example / story:

Ace hardware store in SF. goes in and buys gloves, sweep, spray hose and clorox. Soon as I put items on counter, check-out person said: 'oh you're cleaning your compost bin'

_Can we identify customer intent based on items purchased?_

Customers aren't buying a 1/4 inch drill bit, they're intereste din a 1/4 inch hole. But push it further  - they want to hang a picutre/secure bookcase, etc.

Does domain knowledge (recipes or known projects) improve our abaility to identify projects?

going further: 
Does contextual knowledge (customer demographics) improve our ability to identify projects?


Second example/story:
emergency services wait times. 2 hours. 

In emergency services, wait time is an inverse function: the sicker you are the shorter you wait. 

We all know we need to wait, but what can we do about it? Because at the limit, people can die.

A traditional researcher would look at this and says, oh this is a non-pre-emtive priority queue. I'm going to map this thing out, I'm going to identify the bottlenecks in the process, and design a service impreovement to remove the bottleneck.

Instead of doing this, we said: traditional process design doesn't apply to healthcare, because no patient is a widget. Every patient is unique. So, build sequence algorithm to track each patient, and ultimately try to find a discrimation value and pushes it upstream. Point of care testing - tests that can be performed at the bedside. If you can convert tests to the bedside, recognising tradeoffs in cost vs efficiency, which would you do it for?

Big ideas:

Explore 'why' and 'how' with machine-generated data in additional 'how many' and 'how much'

Approach the analysis of machine-generated data as a complementary, not competing approach
- start with hypothese (semi-supervied methods)
- generate hypotheses (eg. narrow the scope of analysis)

Analyse: "What people do and how systems perform" and not just "What people say"

How can I use data to generate hyphotheses to then turn around and use ethnography to really look into the right areas?

### Marc BÃ¶hlen

![](/images/epic2018/marctitle.JPG)

Can machine learning algorithms detect beauty?

If we try to answer this question in the affirmative, we need an approach and some data. But data is hard to get and hard to make. So, borrow some data: CelebA data set from University of Hong Kong. 200k  face images of celebrities; 40 binary features

![](/images/epic2018/beauty.JPG)

What about approach? The current paradigm is deep learning. i.e. convolutional neural network for image classification.

![](/images/epic2018/CNN.JPG)

3 steps:

1. Data collection - need to be large amounts and high quality. Labels have to be correct and dimensions flattened to fit into framework

2. Training. 

3. Test it. introduce a new image and see what it classifies it. 

So question is: What's difference between testing data and training data? How can you get something that's precise and can be generalised.

Process raises many questions: From network perspective, when do you use a complex framework and when do you use a simple one? From a data perspective, what do you do with noise? i.e. false negatives/positives. Machine learning algorithms are supposed to be immune to that because of generalisation, but we want to 

University of Hong Kong hired 50 workers from mainland China to compile that data. 100 binary decisions per hour per worker. No wonder you're going to have mistakes because it's a rushed job.

There's a deeper problem than just the outsourcing of the labour. When you make these data sets, you are making a world. You are deciding what to include and what not to include. 

There's always slippage. These models get made and they get used for somethign slightly different, and it's hard to track.

These are the video databases in 2018:

![](/images/epic2018/videodata.JPG)

An alterative approach, as an experiment:

Action camera footage as the foundation for  





