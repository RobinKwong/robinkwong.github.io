---
layout: post
title: Epic 2018 day 3 conference notes
date: 2018-10-12 07:00
tags: [conference, ethnography]
permalink: /epic2018-day-three/
---

- [Notes for day two (Thursday)](/epic2018-day-two/)

- [Notes for day one (Wednesday)](/epic2018-day-one/)

![](/images/epic2018/epic.JPG)

## Whatâ€™s Fair in a Data-Mediated World?

Elizabeth Churchill, Astrid Countee, Nathan Good, Miriam Lueck Avery

_This panel will discuss questions of fairness and justice in data-centric systems. While the many social problems caused by data-centric systems are well known, what options are available to us to make things better? Chair Elizabeth Churchill will draw the panelists and audience into conversation about making change on many levels, in our daily work as well as larger-scale collaborations._

What's fair?

- in accordance with rules or statndards; legitimate
- without cheating
- without trying to achieve (or achieving) unjust advantage

Intended and unintended data baises:

- Activity bias
- Data bias
- Sampling bias
- Algorithmic bias
- Interaction bias
- Self-selection bias
- Second order bias
- Interpretation bias

What is evidence and what do we treat as evidence?

ACM code of ethics include: 

2.6 Perform work only inareas of competence

ResponsibleCS.

### Nathan Good

"_We care deeply_" language in disclaimers. We can't say just that anymore; it's become meaningless.

But 'privacy' and 'fairness' mean different things to different people. 
[Asks audience, who shouts out:]

- compliance
- personal safety
- ownership
- transparency

It's not about compliance:

"Every one of your users experiences the UX, no one reads the privacy policy." - Woody Hartzog

Functions that traditionally belong to compliance officers are now shifting to UX. This is a good thing

GDPR very much based around human rights.

Hoxton case study around retail analytics that track how shoppers move around a mall. Instead of tracking and showing everything about the person, only show the feet - most clients only care about the aggregate anyways. 

### Astrid Countee (@ianthro)s

Data for democracy. Became more interested in technology when found that she didn't have access to the team doing work that she was interested in. Became aware of need for the work that ethnographers and anthropologists do - and how more of them need to not just be in a consultative role but be in the room when things get built, and be at the table when decisions get made.

A lot of technologists are aware that they don't have the human insight skills, but they also think that no one else has those skills, [in part] because we're not loud enough about what we do.   

### Miriam Avery

Works at Mozilla. Was at Institute of the Future. 

Most pragmatic resource: [Lean Data Practices](https://www.mozilla.org/en-US/about/policy/lean-data/)

Respect is a pre-requisite for trust. Thinking about what we collect and when we collect it is fundamental.

Provocation: As researchers, I believe that you collect qual and quant data in respectful ways. [However] As researchers in large powerful companies with monopolistic tendancies, the telemtery data that you use to triangulate your data is collected in a coercive way.

### Questions and Opinions

Miriam: mentions case of Roomba collecting spatial data about people's home. Despite this, consumer uptake of Roomba did not diminish. But the example became a case study in advocacy for GDPR regulation, so it all comes back.

Q: I think it's disrupting the team when you are raising all these risks, and it's demoralising the team

Astrid: To shy away from talking about risk is to invite failure. If my job is to make the product better then I'm doing my job when raising these issues. It's important that we as a team can disagree.

Q: What about evidence of all the risk you talk about?

Astrid: We don't have evidence of all the great things the product will do anyways. 

Miriam: Also need to stand up for each other - we cannot fight this alone.

Q: I love shopping, but this whole anonymous thing is not going to give me a discount.

Nathan: Actually given the quality of the data we are probably going to give you the wrong discount anyways.

Q: Transparency

Nathan: Materialisation is a better phrase than transparency. 

Q: Standards committee - why is doing this stuff useful to the company when it takes time away from your day job? 

Astrid: It has actually become easier to justify this recently given some of the blow ups. It's important that individuals in the company feels capable of acting without backlash [so they can do work in advance to prevent disasters from happening].

Q: What's the use of standards and codes if there are no consequences?

Elizabeth: yes, consequences matter, but there also has to be a starting point.

Q: Whose responsibility is it to get researchers in the room?

Astrid: It should be the employer, but often this doesn't happen. So I had to learn what they do and how they talk so I can push back and fight to be in the room. Not necessarily that everyone needs to learn how to code and do data science, but to at least speak the language and not get excluded because of jargon.

Nathan: Examples are helpful. Some places just suck.

Elizabeth: The community can also help each other and provide perspective. Some places suck but many people don't suck - researchers can use empathy to reach out more too.

Comment from the floor: Being approached by researchers 

Comment from the floor: Mutual suspicion (between data science / antrho) starts at university level

Comment from the floor: Hegemony of the techie. 

Comment from the floor: Manager and executives often don't know much about either qualitative OR quantitatve research. 


