---
layout: post
title: iDocs 2018 day 3 conference notes
date: 2018-03-23 09:00
tags: [conference, interactivity, video]
permalink: /idocs-day-3/
---

These are my day 3 (Friday 23 March) notes for [iDocs 2018](https://idocs2018.dcrc.org.uk/), a three day event dedicated to the expanding & evolving field of interactive documentary, in Bristol.

[Day 1 notes](/idocs-day-1/)

[Day 2 notes](/idocs-day-2/)

## Immersive Audioscapes

_Julian Konczak – Tracing Transcendental Tone: work in progress_

_Andrea Diaz – How to create a story-world about Carlos & Alvin?_

_Duncan Speakman – It Must Have Been Dark by Then_

### Julian Konczak

Work centers around the hidden and powerful effects of sacred sound. (i.e. how do you create intended transcendental experiences with digital)

![](/images/i-docs/sacred.JPG)

Pythagoras was interested in the ability to sound to push people into a more sacred space

This psycho-acoustic element has been revisited more recently: La Mont Young and drone choreography

Haunted Media: Media Archaeology and the idea that there is something going on (more esotheric and more etheric than what appears on the surface)

How to deal with multiplicity of platforms and atomisation of audience experinece?

### Andrea Diaz

How to create a story-world about Carlos & Alvin?

![](/images/i-docs/carlosalvin.JPG)

Carlos and Alvin are both blind.
Carlos: from Colombia, 34 years old, lives in Montreal. Works as a translator. grew up in an environment where he was given a lot of freedom and independence to explore as a child

Alvin: from the Bahamas, 32 years old. Also in Montreal. Grew up in an orphanage so was not allowed to be free and independent as a child - was given a cane very early on.

Carlos and Alvin met up. they're both immigrants in their 30s in Montreal; they're both premature babies and became blind because of the same medical error. They use the practice of echolocation to navigate

Could someone who is not blind learn the 'space as a sound language' (echolocation)

Was really hard - even if understood the theory, could not do it in practice.

So started by putting a 360 camera on a helmet for them to wear, and asked them to describe their process as they walked around the city.

It was interesting, but the video was basically awful. So decided to try again in a controlled environment. Chose Alvin's apartment to try this.

Put the film into an HTC Vive

Very excited to to make it, but when people tried it... they felt dizziness and couldn't continue

Decided to solve this by giving two versions of the same video - a stabilised version and Alvin's original version.

Sound:
Originally, had different layers - Voice narration, sound effects, ambient sound etc

Invited Carlos to come - ironic because we had to put the headset on him in order to help him understand VR and 3d digital images and all that.

Validated the audioscape with him. Carlos's reactionw as: Oh that's nice, but when are you going to have the whole sound of the room?' - he means reverb, and ambisonics (full, uninterrupted sphere of sound recorded with a 360 mic)

What's next:
Really want ot have a visualisation of the sound wave in the VR experience - because, otherwise it's so easy to disassociate. 

How do we archive our soundscapes?

### Duncan Speakman - It Must Have Been Dark by Then

![](/images/i-docs/dark.JPG)

[More info about It Must Have Been Dark By Then](https://ambientlit.com/index.php/it-must-have-been-dark-by-then/)

Don't like the term 'immersion' because when working about sound it's always immersive. Instead prefers 'presence'

Implication and Imbrication - overlapping of edges

Anthropocene, Locative Media, Geospatial

What's interesting and somewhat problematic is this relation between text and sound. when you are reading there's no temporality to it - you read 

You have this situation where your reading pace is framed by the audio - sometimes people get frustrated by it because they think they are ahead or behind

Really interested by this idea of giving time frames to printed objects

The reflections acts as a silencing of the world. It almost removes you from the space. It leaves you that time to contemplate the world around you.

Attempts to connect the 'gameplay' (of walking around) with the original text. In Douz, for example, people were not given a choice and


Things the work addresses:

The idea of layered experieneces. On journey out you are choosing places with your head down in the book. On way back, those things become layered. You remember being there half hour ago, but also you remember the story you have read.

Anthroprocene: Problem is that we understand it (and it has been framed)through modes of the visual.

Kanngeiser, 2015

Davis H & Turpin E: Art in the Antrhopocene

The layering of time - Shadowtime. A parallel 

This is augmented reality. This is not trying to create a fiction. (wanted to emphasise that the recording shows things actually happening and how it connects of where you are and what you are doing)

Next stage is to use microphones on the headphones. This allows blending in on live, real-time audio with pre-recorded. So, sound of wind in Bristol could blend into sound of wind in Tunisia. 

Starting from here and starting from where you are right now, and to work outward to other places.

Challenge with interactive work: If you have something to say, why let (make?) the player say it for you?

## The 'What If It' project

[The Iflab](http://www.iflab.net/)

Should we follow a specific methodology when producing an i-doc? What does that look like?

Work of the IF lab is to mix methodologies from multiple disciplines: storytelling, design, programming, etc

The process:

![](/images/i-docs/process.JPG)

Makers are guided through this by cards:

![](/images/i-docs/card.JPG)

And canvases:

![](/images/i-docs/canvas.JPG)

The questions:

Is this empowering or limiting?
Is this the right mix of methodologies?
How do we improve on this?



